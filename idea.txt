Stroomweg — Real-time Netherlands Traffic Intelligence API

════════════════════════════════════════════════════════════════════════

Data Sources

  Real-time feeds (polled every 60s)
    trafficspeed.xml.gz  ~1 MB    opendata.ndw.nu/trafficspeed.xml.gz
    traveltime.xml.gz    ~2.5 MB  opendata.ndw.nu/traveltime.xml.gz

  Reference files (downloaded once, updated occasionally)
    measurement.xml.gz             11 MB   sensor config, road names
    NDW_AVG_Meetlocaties_Shapefile 69 MB   sensor locations with lat/lng

════════════════════════════════════════════════════════════════════════

Phase 1: Data Exploration                                      [DONE]
  Deep understanding of the raw data

  Goal: Understand the structure, volume, and quirks of both NDW feeds
        before writing any production code.

  What we learned (and how it shapes each endpoint):

    ── measurement.xml → GET /sites ────────────────────────────────────

    Raw structure:
      99,324 measurementSiteRecord entries, each with:
        id, road name, lat/lon, lanes (1-7), equipment type, direction,
        and an indexed list of measurementSpecificCharacteristics

    Critical discovery — the index mapping:
      Each site has numbered characteristics that define what each
      measuredValue index means in trafficspeed.xml. Example (3-lane site):
        index 1:  lane1, trafficFlow,  vehicles ≥ 1.85m (all)
        index 2:  lane1, trafficFlow,  vehicles > 2.4m
        index 3:  lane1, trafficFlow,  vehicles > 5.6m  (medium trucks)
        index 4:  lane1, trafficFlow,  vehicles > 11.5m (heavy trucks)
        index 5:  lane1, trafficFlow,  vehicles > 12.2m (long trucks)
        index 6:  lane1, trafficFlow,  all (no filter)
        index 7:  lane1, trafficSpeed, vehicles ≥ 1.85m (all)
        index 8:  lane1, trafficSpeed, vehicles > 2.4m
        ...repeat for lane2, lane3...
      A 3-lane site has 36 indexed values. A 1-lane site has 12.
      This is the Rosetta Stone — without it, speed data is meaningless.

    Endpoint design impact:
      The index mapping is used internally during ingest to extract
      per-lane "all vehicles" speeds. Consumers never see raw indexes —
      the API abstracts this away. /sites just exposes the basics.

      GET /sites/{site_id} response:
      {
        "site_id": "PZH01_MST_0942_01",
        "name": "N209 km 14.500 Li",
        "road": "N209",
        "lat": 52.04056,
        "lon": 4.542184,
        "lanes": 3,
        "equipment": "lus",
        "direction": "southBound",
        "measurement_types": ["trafficFlow", "trafficSpeed"]
      }

    ── trafficspeed.xml → GET /speeds ──────────────────────────────────

    Raw structure:
      19,976 siteMeasurements per snapshot, each with:
        measurementSiteReference (id), measurementTimeDefault (timestamp),
        and N measuredValue entries indexed to match the site's characteristics

      Example for a 3-lane site:
        measuredValue[1]:  TrafficFlow   (lane1, all vehicles)
        measuredValue[7]:  TrafficSpeed  speed=55 km/h (lane1, all vehicles)
        measuredValue[8]:  TrafficSpeed  speed=43 km/h (lane1, vehicles >2.4m)
        measuredValue[13]: TrafficFlow   (lane2, all vehicles)
        measuredValue[19]: TrafficSpeed  speed=61 km/h (lane2, all vehicles)
        ...

    Key findings:
      - Speed values: -1 = no data, 0 = stationary, up to 249 km/h
      - 193K total readings across all sites (multiple per site)
      - Mix of TrafficFlow (vehicles/hr) and TrafficSpeed (km/h)
      - The "all vehicles" class (≥1.85m) is the most useful default

    Endpoint design impact:
      Most consumers want ONE speed per site, not 12-36 indexed values.
      The API should aggregate by default and offer detail as opt-in.

      GET /speeds (default — one speed per site):
      {
        "site_id": "PZH01_MST_0942_01",
        "timestamp": "2026-02-16T06:18:00Z",
        "speed_kmh": 53,              ← average across lanes, "all vehicles" class
        "flow_veh_hr": 1560,          ← total flow across lanes
        "road": "N209",
        "lat": 52.04056,
        "lon": 4.542184
      }

      GET /speeds/{site_id}?detail=lanes (per-lane breakdown):
      {
        "site_id": "PZH01_MST_0942_01",
        "timestamp": "2026-02-16T06:18:00Z",
        "lanes": [
          { "lane": 1, "speed_kmh": 55, "flow_veh_hr": 480 },
          { "lane": 2, "speed_kmh": 61, "flow_veh_hr": 540 },
          { "lane": 3, "speed_kmh": 43, "flow_veh_hr": 540 }
        ]
      }

      Ingest decision: store per-lane readings in DB (full granularity),
      aggregate at query time for the default response. Don't lose data.
      Schema: see speeds_raw in Phase 2 TimescaleDB Schema.

      Filtering out bad data:
        -1 → null (omit from response and from averages)
        speed > 200 → flag as suspect, still store but exclude from avg

    ── traveltime.xml → GET /journey-times ─────────────────────────────

    Raw structure:
      79,336 siteMeasurements per snapshot, each with ONE measuredValue:
        TravelTimeData containing:
          travelTimeType: "reconstituted"
          duration: 112.5 sec (actual travel time)
          accuracy: 100.0
          supplierCalculatedDataQuality: 58.3 (0-100 quality score)
          numberOfInputValuesUsed: 57
        Plus a reference value:
          duration: 99.9 sec (free-flow baseline)

    Key findings:
      - Much simpler than speeds: one reading per segment, not per-lane
      - Quality score tells you how reliable the reading is
      - Reference duration is NDW's free-flow baseline (useful for Phase 4)
      - -1 = no data
      - Segments are short (median 20 sec) — they're between sensor pairs

    Endpoint design impact:
      Straightforward mapping. Add computed delay fields.

      GET /journey-times response:
      {
        "site_id": "PGL03_GV_N845_3_1_NIEUW",
        "timestamp": "2026-02-16T06:19:00Z",
        "duration_sec": 112.5,
        "reference_duration_sec": 99.9,     ← free-flow baseline from NDW
        "delay_sec": 12.6,                  ← computed: actual - reference
        "delay_ratio": 1.13,                ← computed: actual / reference
        "accuracy": 100.0,
        "quality": 58.3,
        "input_values": 57,
        "road": "N845",
        "lat": 51.92,
        "lon": 5.87
      }

      Schema: see journey_times_raw in Phase 2 TimescaleDB Schema.

      Quality filtering: consumers can use ?min_quality=50 to exclude
      low-confidence readings.


════════════════════════════════════════════════════════════════════════

Phase 2: Ingest Pipeline
  Database filling up with live traffic data 24/7

  What it does:
    - Polls both NDW feeds every 60 seconds
    - Parses DATEX II XML in memory (never stored as files)
    - Writes extracted values to database
    - Loads reference data (measurement.xml + shapefile) on startup
    - Runs 24/7, handles NDW downtime gracefully

  Data flowing in:
    Speeds:        ~40K rows/minute  (~58M rows/day)
                   (20K sites × avg ~2 lanes, one row per site/lane)
    Journey times: ~80K rows/minute  (~115M rows/day)
                   (79K segments, one row per segment)

  ── TimescaleDB Schema ─────────────────────────────────────────────

  1. sites (regular PostgreSQL table — reference data)
     ┌────────────────────┬──────────────┬─────────────────────────────┐
     │ Column             │ Type         │ Notes                       │
     ├────────────────────┼──────────────┼─────────────────────────────┤
     │ site_id            │ TEXT PK      │ e.g. "PZH01_MST_0942_01"   │
     │ name               │ TEXT         │ "N209 km 14.500 Li"         │
     │ road               │ TEXT         │ extracted: "N209"           │
     │ lanes              │ SMALLINT     │ 1-7                         │
     │ equipment          │ TEXT         │ lus, fcd, anpr, radar       │
     │ direction          │ TEXT         │ northBound, southBound, etc │
     │ geom               │ GEOMETRY     │ PostGIS POINT(lon, lat)     │
     │ municipality       │ TEXT         │ from shapefile spatial join  │
     │ has_speed          │ BOOLEAN      │ site appears in trafficspeed│
     │ has_travel_time    │ BOOLEAN      │ site appears in traveltime  │
     │ index_mapping      │ JSONB        │ measuredValue index → lane/ │
     │                    │              │ type mapping (Rosetta Stone) │
     │ updated_at         │ TIMESTAMPTZ  │ last reference data refresh │
     └────────────────────┴──────────────┴─────────────────────────────┘
     Indexes:
       GiST on geom         → bbox queries: ST_Within(geom, bbox)
       btree on road         → ?road=A28 filtering
       btree on municipality → ?municipality=Utrecht filtering
     Source: measurement.xml + shapefile (loaded on startup)
     ~99K rows, refreshed daily/weekly

  2. speeds_raw (TimescaleDB hypertable)
     ┌────────────────────┬──────────────┬─────────────────────────────┐
     │ Column             │ Type         │ Notes                       │
     ├────────────────────┼──────────────┼─────────────────────────────┤
     │ timestamp          │ TIMESTAMPTZ  │ measurementTimeDefault      │
     │ site_id            │ TEXT         │ references sites (no FK)    │
     │ lane               │ SMALLINT     │ lane number (1, 2, 3...)    │
     │ speed_kmh          │ REAL         │ null if -1 (no data)        │
     │ flow_veh_hr        │ INTEGER      │ null if -1 (no data)        │
     └────────────────────┴──────────────┴─────────────────────────────┘
     Hypertable config:
       chunk_interval    → 6 hours (58M rows/day ÷ 4 = ~15M per chunk)
       unique constraint → (timestamp, site_id, lane)
     Indexes:
       btree on (site_id, timestamp DESC) → per-site history queries
       default TimescaleDB time index     → time-range scans
     Only "all vehicles" class (≥1.85m) stored — vehicle-class breakdown
     discarded during ingest to keep volume manageable.
     No DB-level FK on site_id — validated in application during ingest.
     FK checks on 40K+ inserts/min would add unnecessary overhead.

  3. journey_times_raw (TimescaleDB hypertable)
     ┌────────────────────┬──────────────┬─────────────────────────────┐
     │ Column             │ Type         │ Notes                       │
     ├────────────────────┼──────────────┼─────────────────────────────┤
     │ timestamp          │ TIMESTAMPTZ  │ measurementTimeDefault      │
     │ site_id            │ TEXT         │ references sites (no FK)    │
     │ duration_sec       │ REAL         │ null if -1                  │
     │ ref_duration_sec   │ REAL         │ free-flow baseline          │
     │ accuracy           │ REAL         │ 0-100                       │
     │ quality            │ REAL         │ supplierCalculatedDataQual  │
     │ input_values       │ INTEGER      │ numberOfInputValuesUsed     │
     └────────────────────┴──────────────┴─────────────────────────────┘
     Hypertable config:
       chunk_interval    → 3 hours (115M rows/day ÷ 8 = ~14M per chunk)
       unique constraint → (timestamp, site_id)
     Indexes:
       btree on (site_id, timestamp DESC) → per-segment history queries
       default TimescaleDB time index     → time-range scans

  4. Continuous aggregates (materialized views, auto-refreshed)

     speeds_5m:
       SELECT time_bucket('5 min', timestamp) AS bucket,
              site_id, lane,
              AVG(speed_kmh)   AS avg_speed_kmh,
              AVG(flow_veh_hr) AS avg_flow_veh_hr
       FROM speeds_raw
       GROUP BY bucket, site_id, lane
       -- AVG not SUM: flow is a rate (veh/hr), not a count

     speeds_15m, speeds_1h: same structure, different bucket size

     journey_times_5m:
       SELECT time_bucket('5 min', timestamp) AS bucket,
              site_id,
              AVG(duration_sec)     AS avg_duration_sec,
              AVG(ref_duration_sec) AS avg_ref_duration_sec,
              AVG(quality)          AS avg_quality
       FROM journey_times_raw
       GROUP BY bucket, site_id

     journey_times_15m, journey_times_1h: same structure, different bucket

     Refresh policy: each aggregate refreshes on a lag
       5m agg  → refresh every 5 min,  covers last 15 min
       15m agg → refresh every 15 min, covers last 1 hour
       1h agg  → refresh every 1 hour, covers last 4 hours

  5. Retention policies (automatic chunk drops)
     ┌─────────────────────┬───────────┬──────────────────────────────┐
     │ Table               │ Keep      │ Rows at steady state         │
     ├─────────────────────┼───────────┼──────────────────────────────┤
     │ speeds_raw          │ 7 days    │ ~406M rows                   │
     │ speeds_5m           │ 30 days   │ ~346M rows                   │
     │ speeds_15m          │ 90 days   │ ~346M rows                   │
     │ speeds_1h           │ forever   │ grows ~960K rows/day         │
     ├─────────────────────┼───────────┼──────────────────────────────┤
     │ journey_times_raw   │ 7 days    │ ~805M rows                   │
     │ journey_times_5m    │ 30 days   │ ~683M rows                   │
     │ journey_times_15m   │ 90 days   │ ~683M rows                   │
     │ journey_times_1h    │ forever   │ grows ~1.9M rows/day         │
     └─────────────────────┴───────────┴──────────────────────────────┘
     Row count math:
       40K unique (site_id, lane) combos × buckets/day × retention days
       79K unique site_id combos × buckets/day × retention days
       buckets/day: 5m=288, 15m=96, 1h=24

     Total storage at steady state (estimated):
       Raw (7d):   ~1.2B rows  → ~40 GB (assuming ~30 bytes/row avg)
       5m (30d):   ~1.03B rows → ~35 GB
       15m (90d):  ~1.03B rows → ~35 GB
       1h (all):   grows ~2.9M rows/day → ~32 GB/year
       Sites:      ~99K rows   → negligible

  6. api_keys (regular PostgreSQL table — Phase 5)
     ┌────────────────────┬──────────────┬─────────────────────────────┐
     │ Column             │ Type         │ Notes                       │
     ├────────────────────┼──────────────┼─────────────────────────────┤
     │ id                 │ UUID PK      │                             │
     │ key_hash           │ TEXT UNIQUE  │ SHA-256 of the API key      │
     │ email              │ TEXT         │ owner                       │
     │ tier               │ TEXT         │ free, pro                   │
     │ rate_limit         │ INTEGER      │ requests per minute         │
     │ created_at         │ TIMESTAMPTZ  │                             │
     │ last_used_at       │ TIMESTAMPTZ  │                             │
     │ active             │ BOOLEAN      │ soft delete                 │
     └────────────────────┴──────────────┴─────────────────────────────┘

  ── How the ingest pipeline writes ────────────────────────────────

  Every 60 seconds, per feed:
    1. Fetch gzipped XML from NDW (HTTP GET)
    2. Decompress + parse in memory (lxml iterparse, stream-based)
    3. For each siteMeasurements:
       - Look up site's index_mapping from sites table (cached in memory)
       - Extract per-lane speed/flow OR travel time values
       - Skip -1 values (store as null)
    4. Batch INSERT into hypertable (use COPY for speed, or
       INSERT ... ON CONFLICT DO NOTHING for idempotency)
    5. Publish latest snapshot to Redis (cache + pub/sub)

  Idempotency: ON CONFLICT (timestamp, site_id, lane) DO NOTHING
  ensures re-processing the same snapshot is safe.

  Backpressure: if DB write takes > 60s, skip the next poll cycle
  and log a warning. Don't queue up — stale data is worse than
  missing data.

  Reference data refresh:
    - measurement.xml parsed into sites table on startup
    - Shapefile loaded for lat/lng + municipality mapping
    - Refreshed on a schedule (daily or weekly)

════════════════════════════════════════════════════════════════════════

Phase 3: REST API + WebSocket Streams
  Working API serving real Netherlands traffic data

  ── Sites — sensor discovery ──────────────────────────────────────
  ┌──────────────────────────────┬──────────────────────────────────┐
  │ GET /sites                   │ All sites with location, road,   │
  │                              │ lanes, equipment type             │
  │                              │ ?bbox=...  ?road=...             │
  │                              │ ?municipality=...                │
  ├──────────────────────────────┼──────────────────────────────────┤
  │ GET /sites/{site_id}         │ Single site detail               │
  └──────────────────────────────┴──────────────────────────────────┘

  ── Speeds — real-time + snapshot + historical ────────────────────
  ┌──────────────────────────────┬──────────────────────────────────┐
  │ GET /speeds/stream           │ SSE — speed updates every 60s    │
  │                              │ ?bbox=...  ?road=...  ?site_id=…│
  │                              │ Requires at least one filter     │
  ├──────────────────────────────┼──────────────────────────────────┤
  │ GET /speeds                  │ Snapshot — latest at every site  │
  │                              │ ?bbox=...  ?road=...             │
  │                              │ Requires at least one filter     │
  │                              │ (20K sites too large unfiltered) │
  ├──────────────────────────────┼──────────────────────────────────┤
  │ GET /speeds/{site_id}        │ Current speed at one site        │
  │                              │ ?detail=lanes (per-lane detail)  │
  ├──────────────────────────────┼──────────────────────────────────┤
  │ GET /speeds/{site_id}/history│ Historical readings              │
  │                              │ ?from=...&to=...                 │
  │                              │ (default: last 1 hour)           │
  │                              │ ?resolution=1m|5m|15m|1h         │
  └──────────────────────────────┴──────────────────────────────────┘
  Note: ?congested=true added in Phase 4 once baselines exist

  ── Journey Times — real-time + snapshot + historical ─────────────
  ┌──────────────────────────────┬──────────────────────────────────┐
  │ GET /journey-times/stream    │ SSE — travel time updates / 60s  │
  │                              │ ?bbox=...  ?road=...  ?site_id=…│
  │                              │ Requires at least one filter     │
  ├──────────────────────────────┼──────────────────────────────────┤
  │ GET /journey-times           │ Snapshot — all route segments    │
  │                              │ ?bbox=...  ?road=...             │
  │                              │ ?min_quality=50 (filter by NDW   │
  │                              │   data quality score, 0-100)     │
  │                              │ Includes actual vs free-flow     │
  │                              │ Requires at least one filter     │
  │                              │ (80K segs too large unfiltered)  │
  ├──────────────────────────────┼──────────────────────────────────┤
  │ GET /journey-times/{site_id} │ Current travel time, one segment │
  ├──────────────────────────────┼──────────────────────────────────┤
  │ GET /journey-times/{site_id}/│ Historical travel times          │
  │   history                    │ ?from=...&to=...                 │
  │                              │ (default: last 1 hour)           │
  │                              │ ?resolution=1m|5m|15m|1h         │
  └──────────────────────────────┴──────────────────────────────────┘

  ── WebSocket — bidirectional multi-feed ──────────────────────────
  ┌──────────────────────────────┬──────────────────────────────────┐
  │ WS /ws                       │ Single connection, dynamic subs  │
  │                              │ → { subscribe: "speeds",         │
  │                              │     bbox: "52.3,4.8,52.4,5.0" } │
  │                              │ → { unsubscribe: "speeds" }      │
  │                              │ → { subscribe: "journey-times",  │
  │                              │     road: "A28" }                │
  │                              │ ← server pushes matching events  │
  └──────────────────────────────┴──────────────────────────────────┘

  ── Health — monitoring ────────────────────────────────────────────
  ┌──────────────────────────────┬──────────────────────────────────┐
  │ GET /health                  │ Service status, last successful  │
  │                              │ poll time, data staleness,       │
  │                              │ DB connection, Redis connection   │
  └──────────────────────────────┴──────────────────────────────────┘

  Shared query parameters (consistent across all list endpoints):
    bbox          lat1,lon1,lat2,lon2
    road          A28, N201, etc.
    municipality  Utrecht, etc.

  Pagination (on list endpoints):
    limit         max results (default 100, max 1000)
    offset        skip N results
    Response includes: total_count, limit, offset, next_url

════════════════════════════════════════════════════════════════════════

Phase 4: Congestion Intelligence
  Unique derived value that NDW doesn't offer

  NDW gives raw numbers. Stroomweg tells you what they mean.

  ?congested=true on /speeds and /speeds/stream
    - Compares current speed against historical baseline for that site,
      time of day, and day of week
    - Returns congestion_score (0-100) and severity (free_flow, slow,
      congested, gridlock)
    - Requires Phase 2 running long enough to build baselines
      (minimum ~1 week of data, improves over time)

  Congestion fields added to speed responses when ?congested=true:
    {
      "site_id": "RWS01_MONIBAS_0161hrr0346ra",
      "speed_kmh": 35,
      "typical_speed_kmh": 95,
      "congestion_score": 78,
      "severity": "congested"
    }

  Journey time equivalent:
    - delay_sec and delay_ratio already computed in Phase 3
    - Phase 4 adds: severity label (free_flow, slow, congested, gridlock)
      based on delay_ratio thresholds and historical patterns

  Future ideas (not MVP):
    - Congestion predictions ("A28 typically clears by 09:15")
    - Alerting: "notify me when A28 speed drops below 50"

════════════════════════════════════════════════════════════════════════

Phase 5: Developer Experience
  Shareable product with docs and signup

  API Keys & Auth
    - All endpoints require X-API-Key header
    - Self-service signup: email → verify → get key
    - Usage tiers: free (100 req/min), pro (1000 req/min)

  Rate Limiting
    - Per-key limits based on tier
    - Streaming connections: 1 request per update pushed
    - Headers: X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset
    - 429 Too Many Requests with Retry-After

  Documentation
    - Interactive API docs (OpenAPI/Swagger)
    - Getting started guide with curl examples
    - WebSocket connection examples
    - Code samples (Python, JavaScript, curl)

  Developer Portal
    - Signup / key management
    - Usage dashboard (requests, bandwidth, errors)
    - Status page (is the ingest pipeline healthy?)

════════════════════════════════════════════════════════════════════════

Design Choices & Justification

  Language: Python
    Why: XML parsing ecosystem (lxml, ElementTree) is mature. Data was
    already explored in Python. Async support via asyncio is sufficient
    for the concurrency model (long-lived SSE/WS connections + periodic
    polling). Not CPU-bound — the bottleneck is I/O (network to NDW,
    writes to DB), so Python's speed is fine.
    Why not Go/Rust: Overkill. The ingest pipeline processes ~3.5 MB of
    gzipped XML per minute — not a performance-critical workload.

  Framework: FastAPI
    Why: Async-native (ASGI), built-in WebSocket support, auto-generated
    OpenAPI docs (Phase 5 for free), Pydantic for request/response
    validation. SSE via sse-starlette.
    Why not Django: Too heavy, sync-first, Django REST Framework adds
    complexity without benefit for a data API with no ORM-driven CRUD.
    Why not Flask: No async, no WebSocket, no auto docs.

  Database: PostgreSQL + TimescaleDB
    Why: Time-series is the core data model (speed readings every 60s
    per site). TimescaleDB gives automatic partitioning (hypertables),
    built-in rollup functions (time_bucket), and retention policies —
    exactly what Phase 2 needs.
    Why not plain PostgreSQL: ~58M speed rows/day = ~21B rows/year.
    Regular tables would require manual partitioning and struggle with
    time-range queries at this scale.
    Why not InfluxDB: No spatial queries. Would need a separate DB for
    site metadata and geo lookups. TimescaleDB keeps everything in one
    place with full SQL.
    Why TimescaleDB + PostGIS together: One database handles both
    time-series queries (speeds over the last hour) AND spatial queries
    (sites within this bounding box). No need to join across systems.

  Caching: Redis
    Why: Three roles —
    1. Latest state cache: store the most recent snapshot of all speeds
       and journey times. GET /speeds reads from Redis, not DB.
       Avoids hitting TimescaleDB for "what's happening right now?"
    2. Pub/Sub: ingest service publishes updates to Redis channels.
       SSE/WS handlers subscribe and push to connected clients.
       Decouples ingest from API processes.
    3. Rate limiting: track per-key request counts with TTL-based keys.

  Architecture: Separate ingest + API processes
    ┌─────────┐     ┌───────────┐     ┌──────────────┐
    │   NDW   │────→│  Ingest   │────→│ TimescaleDB  │
    │ (poll)  │     │  Service  │     │  + PostGIS   │
    └─────────┘     └─────┬─────┘     └──────┬───────┘
                          │                  │
                          │ publish           │ query
                          ▼                  │
                    ┌───────────┐             │
                    │   Redis   │             │
                    │ pub/sub + │             │
                    │   cache   │             │
                    └─────┬─────┘             │
                          │ subscribe         │
                          ▼                  ▼
                    ┌───────────────────────────┐
                    │      FastAPI (API)         │
                    │  REST / SSE / WebSocket    │
                    └───────────────────────────┘
    Why separate: If the API crashes or restarts, ingestion continues.
    If ingestion stalls, the API still serves cached/historical data.
    They scale independently — one ingest process is enough, but you
    might need multiple API workers behind a load balancer.

  Streaming: SSE + WebSocket
    SSE for: Simple one-way feeds. Client opens a connection, gets
    updates pushed. Auto-reconnect with Last-Event-ID. Works through
    proxies and CDNs. Enough for "show me live speeds in this bbox."
    WebSocket for: Bidirectional. Client can subscribe/unsubscribe to
    multiple topics on one connection without reconnecting. Power users,
    dashboards, multi-feed use cases.
    Why not WebSocket only: SSE is simpler to implement, debug, and
    cache. Most consumers just need a one-way stream. WebSocket adds
    complexity (connection state, heartbeats, reconnect logic).

  Spatial: PostGIS on PostgreSQL
    Why: bbox queries are core to every list endpoint. PostGIS spatial
    indexes (GiST) make "find all sites within this bbox" fast on 99K
    sites. Also enables future features like radius queries, nearest-
    site lookup, and corridor queries (sites along a road geometry).
    Why not application-level filtering: Scanning 99K sites in Python
    for every bbox query is slow. Let the database do it with an index.

  Auth: API keys (not OAuth)
    Why: This is a data API. No user sessions, no login flows, no
    scopes. A static key in the X-API-Key header is the simplest model
    that works. Matches how every other data API works (Google Maps,
    OpenWeather, etc.).
    Keys stored: hashed in PostgreSQL, looked up per request (cached
    in Redis for speed).

  Data Retention: Tiered rollup
    raw 1m  → keep 7 days     (full resolution, recent data)
    5m avg  → keep 30 days    (still useful for dashboards)
    15m avg → keep 90 days    (trend analysis)
    1h avg  → keep forever    (long-term baselines for Phase 4)
    Why: ~173M rows/day combined (speeds + journey times) at raw is
    unsustainable long-term. Aggregation preserves trends while keeping
    storage bounded. TimescaleDB continuous aggregates handle rollups
    automatically.

  API Design:
    - Congestion is a filter (?congested=true), not a separate endpoint
    - History on both real-time resources (speeds + journey times)
    - /sites exists so consumers can discover valid site_ids
    - Consistent filtering (bbox, road, municipality) on every list
    - resolution param on history endpoints matches retention tiers
    - XML never touches disk — parsed in memory, values go to DB
    - Phase 2 starts early so baselines accumulate for Phase 4
